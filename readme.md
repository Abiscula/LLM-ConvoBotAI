# LLM-ConvoBotAI

Este √© um projeto de chatbot AI que utiliza modelos locais de linguagem (LLM) para conversas cont√≠nuas, mantendo o contexto da sess√£o. Ou seja, durante uma conversa, o chatbot consegue ‚Äúlembrar‚Äù das intera√ß√µes anteriores, permitindo uma experi√™ncia mais fluida e natural.

Al√©m disso, o LLM-ConvoBotAI agora conta com um novo fluxo interativo que permite ao usu√°rio escolher entre:
‚Ä¢ Conversar com o assistente (modo tradicional de chatbot)
‚Ä¢ Analisar um arquivo/documento com a ajuda da IA (modo an√°lise)

Esse diferencial torna o projeto mais flex√≠vel e poderoso, permitindo intera√ß√µes tanto informais quanto an√°lises de conte√∫do mais estruturadas.

![Exemplo de Prompt](./prompt-example.png)

## üß† Diferen√ßas para o projeto anterior

Diferente do projeto [AI-chatbot](https://github.com/Abiscula/AI-chatbot) que implementei com o Groq, este oferece maior flexibilidade na escolha do modelo de linguagem, permitindo ao usu√°rio selecionar entre uma ampla variedade de LLMs (como GPT-3, Phi-3, entre outros), conforme suas necessidades.

O suporte a diferentes provedores ‚Äî Ollama, Hugging Face, e modelo manual local ‚Äî permite que o chatbot seja executado em diversos sistemas operacionais com liberdade total de configura√ß√£o.

Al√©m disso, o novo sistema de fluxo do assistente permite que o usu√°rio:
‚Ä¢ Escolha entre iniciar uma conversa tradicional com o chatbot ou fazer a leitura e an√°lise de um arquivo (como PDFs, textos, etc.).
‚Ä¢ Utilize o mesmo modelo de linguagem em ambos os fluxos.
‚Ä¢ Mantenha o contexto da sess√£o para intera√ß√µes mais inteligentes.

## üõ† Tecnologias Utilizadas

- **Python**: Linguagem de programa√ß√£o principal.
- **Ollama**: Plataforma para executar LLMs localmente.
- **Phi-3 (quantized)**: Modelo de linguagem para ser carregado manualmente.
- **Streamlit**: Framework para criar a interface interativa do chatbot.
- **LangChain**: Biblioteca usada para integrar LLMs e orquestrar conversas.
- **Hugging Face**: Para usar modelos pr√©-treinados via API.

## üöÄ Como Rodar o Projeto

### 1. Pr√©-requisitos

- **Python 3.8+**
- **Pip** para instalar as depend√™ncias
- **Ollama**: Caso deseje utilizar o Ollama como backend para o modelo local
- **Hugging Face API Key**: Caso deseje usar modelos da Hugging Face

### 2. Configurando o Ambiente

#### 1. Clone este reposit√≥rio para o seu ambiente local:

```bash
git clone <URL_DO_REPOSITORIO>
cd <DIRETORIO_DO_REPOSITORIO>
```

#### 2. Crie um ambiente virtual:

```bash
 python3 -m venv venv
 source venv/bin/activate  # Para Linux/MacOS
 venv\Scripts\activate     # Para Windows
```

#### 3. Instale as depend√™ncias do projeto:

```bash
 pip install -r requirements.txt
```

#### 4. Configure a Hugging Face API Key:

Para usar modelos da Hugging Face, voc√™ precisa configurar a chave da API. Crie um arquivo .env no diret√≥rio raiz do projeto e adicione a seguinte linha:

```bash
 HF_API_KEY=seu_token_da_hugging_face
```

Substitua seu_token_da_hugging_face pelo seu token da API da Hugging Face, que pode ser obtido em Hugging Face - API.

### 3. Rodando o Modelo

#### Op√ß√£o 1: Usando Ollama

Para usar o Ollama, voc√™ precisar√° instalar o aplicativo Ollama localmente.

1.  Instale o Ollama:
    ‚Ä¢ Para Windows: Fa√ßa o download e instale o Ollama a partir do site oficial do Ollama.
    ‚Ä¢ Para MacOS e Linux: Utilize o Homebrew para instalar o Ollama:

```bash
 brew install ollama
```

2.  Inicie o servidor Ollama:

Execute o seguinte comando para iniciar o servidor Ollama em sua m√°quina:

```bash
 ollama serve
```

#### Op√ß√£o 2: Usando o Modelo Manual (para Windows)

Caso esteja em um dispositivo Windows ou n√£o queira usar o Ollama, voc√™ pode optar pelo modelo manual (Phi-3).

1. Baixe o modelo Phi-3 quantizado:
   ‚Ä¢ Baixe o modelo manualmente e coloque-o em um diret√≥rio espec√≠fico dentro do seu projeto.
2. Rodando o chatbot com Phi-3:
   O modelo ser√° carregado manualmente a partir do diret√≥rio onde voc√™ o colocou.
3. Iniciar o chatbot:
   Quando voc√™ executar o c√≥digo, o modelo manual ser√° utilizado no lugar do Ollama.

#### Op√ß√£o 3: Usando Hugging Face

Se voc√™ preferir usar um modelo pr√©-treinado da Hugging Face, voc√™ pode configurar o projeto para se conectar √† API da Hugging Face.

1. Certifique-se de que o arquivo .env est√° configurado com a sua API Key (conforme a etapa 4 do item anterior).
2. Escolha um modelo na Hugging Face:
   V√° at√© Hugging Face Models e escolha um modelo de sua prefer√™ncia.
3. Execute o chatbot:
   Com a API configurada, basta rodar o chatbot com o modelo desejado. O c√≥digo ir√° automaticamente se conectar √† Hugging Face para carregar o modelo e utiliz√°-lo para gerar as respostas.

## üñ•Ô∏è Compatibilidade

| Modelo usado  | Windows | MacOS | Linux |
| ------------- | ------- | ----- | ----- |
| Ollama        | ‚úÖ      | ‚úÖ    | ‚úÖ    |
| Modelo Manual | ‚úÖ      | ‚ùå    | ‚ùå    |
| Hugging Face  | ‚úÖ      | ‚úÖ    | ‚úÖ    |
